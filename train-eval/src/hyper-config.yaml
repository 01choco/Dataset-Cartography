# train
train : True
type : dpo # dpo / simpo
model : llama3-8b # mistral-7b / llama3-8b
epoch : 3.0

# dpo config (llamafactory)
save_path : saves/${model}/${type}
yaml_path : examples/train_lora/${model}/${type}
avail_devices : 0,3
start : 6 #이 숫자부터 진행 
end : 12 # 이 숫자까지 진행

# simpo config (simpo)
# per_device_train_batch_size: 4
# gradient_accumulation_steps: 1

# export
export : True
export_yaml_path : examples/merge_lora/${model}/${type}
model_path : models/${model}/${type}

# export config
epoch_list : [] # 중간 epoch eval할 경우 
last_ckpt : True # 훈련 종료 후 마지막 checkpoint eval할 경우
last_ckpt_epoch : 3 # 마지막 checkpoint eval할 경우 epoch

# eval
eval : False
judge_name : gpt-4o-2024-11-20
judge_device : 0
judge_device_2 : 3
parallel_eval : True

# evaluation metric config
mt_bench : True
evol_instruct : True
alpaca_eval : False
hhh : False

# eval option config 
mt_only_gpt : False
evol_only_gpt : False
mt_only_show : False
evol_only_show : False

# delete
delete : False